{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotion Feature Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "def smooth_emotions2(emotion_series, window_size=10, min_duration=10):\n",
    "    \"\"\"\n",
    "    Smooth emotion sequence using rule-based method\n",
    "    window_size: Sliding window size\n",
    "    min_duration: Minimum duration frames\n",
    "    \"\"\"\n",
    "    emotions = emotion_series.values\n",
    "    smoothed = emotions.copy()\n",
    "    n = len(emotions)\n",
    "\n",
    "    # 1. Use sliding window majority voting for initial smoothing\n",
    "    for i in range(n):\n",
    "        start = max(0, i - window_size // 2)\n",
    "        end = min(n, i + window_size // 2 + 1)\n",
    "        window = emotions[start:end]\n",
    "\n",
    "        # If current frame is valid emotion, perform majority voting smoothing\n",
    "        if emotions[i] != 'undefined':  # Only process when current emotion is not 'undefined'\n",
    "            # Get valid emotions in window\n",
    "            valid_emotions = window[window != 'undefined']\n",
    "            \n",
    "            # If there are valid emotions, perform majority voting\n",
    "            if len(valid_emotions) > 0:\n",
    "                unique, counts = np.unique(valid_emotions, return_counts=True)\n",
    "                majority = unique[counts.argmax()]\n",
    "                smoothed[i] = majority\n",
    "            else:\n",
    "                smoothed[i] = emotions[i]\n",
    "        else:\n",
    "            smoothed[i] = emotions[i]  # No processing for undefined emotions\n",
    "\n",
    "    # 2. Split emotions with duration less than min_duration frames, loop until no segments smaller than min_duration\n",
    "    while True:\n",
    "        i = 0\n",
    "        modified = False  # Record whether any modifications were made\n",
    "        while i < n - 1:\n",
    "            # Find consecutive same emotion segments\n",
    "            start = i\n",
    "            while i < n - 1 and smoothed[i] == smoothed[i + 1]:\n",
    "                i += 1\n",
    "            end = i + 1  # Current emotion segment end position\n",
    "            \n",
    "            # If current segment duration is less than min_duration, split it\n",
    "            if end - start < min_duration:\n",
    "                # Determine previous and next emotion categories\n",
    "                if start > 0:\n",
    "                    prev_emotion = smoothed[start - 1]\n",
    "                else:\n",
    "                    prev_emotion = 'undefined'\n",
    "\n",
    "                if end < n:\n",
    "                    next_emotion = smoothed[end] \n",
    "                else:\n",
    "                    next_emotion = 'undefined'\n",
    "\n",
    "                # Split current segment, first half goes to previous emotion, second half goes to next emotion\n",
    "                mid = start + (end - start) // 2\n",
    "                smoothed[start:mid] = prev_emotion\n",
    "                smoothed[mid:end] = next_emotion\n",
    "\n",
    "                modified = True  # Mark as modified\n",
    "                break  # Exit loop for next round of checking\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        # If no modifications were made, splitting is complete, exit loop\n",
    "        if not modified:\n",
    "            break\n",
    "\n",
    "    return pd.Series(smoothed, index=emotion_series.index)\n",
    "\n",
    "def process_and_save_smoothed_emotions(input_dir, output_dir, window_size=10, min_duration=10):\n",
    "    \n",
    "    # Create output directory\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    try:\n",
    "        # Get all CSV files\n",
    "        csv_files = [f for f in os.listdir(input_dir) if f.endswith('.csv')]\n",
    "        if not csv_files:\n",
    "            raise ValueError(f\"在 {input_dir} 中没有找到CSV文件\")\n",
    "            \n",
    "        print(f\"开始处理{len(csv_files)}个文件的数据...\")\n",
    "        \n",
    "        # Process all files\n",
    "        for i, csv_file in enumerate(csv_files, 1):\n",
    "            input_file = os.path.join(input_dir, csv_file)\n",
    "            output_file = os.path.join(output_dir, csv_file)\n",
    "            \n",
    "            print(f\"\\r处理进度：{i}/{len(csv_files)} ({i/len(csv_files)*100:.1f}%) - \"\n",
    "                  f\"当前处理：{csv_file}\", end='')\n",
    "            # if csv_file != '白硕旻.csv':\n",
    "            #     continue\n",
    "            try:\n",
    "                # Read first two header lines\n",
    "                with open(input_file, 'r') as f:\n",
    "                    header_lines = [next(f) for _ in range(2)]\n",
    "                \n",
    "                # Read data\n",
    "                df = pd.read_csv(input_file, skiprows=2)\n",
    "                \n",
    "                # Smooth emotion sequence\n",
    "                df['SmoothedEmotion'] = smooth_emotions2(df['Emotion'], window_size, min_duration)\n",
    "                \n",
    "                # Save results, including first two header lines\n",
    "                with open(output_file, 'w', newline='') as f:\n",
    "                    # Write original two header lines\n",
    "                    f.writelines(header_lines)\n",
    "                    f.write('\\n')\n",
    "                    # Write processed data\n",
    "                    df.to_csv(f, index=False)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\n警告：处理 {csv_file} 时出错: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        print(\"\\n所有数据处理完成，平滑后的结果已保存至 result/Emotions_smooth/\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"错误：{str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Execute processing\n",
    "# process_and_save_smoothed_emotions(input_dir=\"result/Emotions\", output_dir=\"result/Emotions_smooth2\", window_size=10, min_duration=10)\n",
    "# process_and_save_smoothed_emotions(input_dir=\"result/Emotions\", output_dir=\"result/Emotions_smooth3\", window_size=5, min_duration=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Extract features\n",
    "def analyze_emotion_features(emotion_dir, feature_type, remove_outlier=True, demographic_csv=r'result/demographics.csv', output_dir=\"result/Emotion_features\", is_save = True):\n",
    "    # Define constants\n",
    "    EMOTIONS = ['neutral', 'happy', 'sad', 'surprise', 'anger']\n",
    "    FRAME_DURATION = 20  # 20 milliseconds per frame\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Read emotion CSV data\n",
    "    csv_files = [f for f in os.listdir(emotion_dir) if f.endswith('.csv')]\n",
    "    print(f\"开始处理{len(csv_files)}个文件...\")\n",
    "\n",
    "    # Store results for all subjects\n",
    "    results = []\n",
    "\n",
    "    for file in csv_files:\n",
    "        try:\n",
    "            # Get subject name\n",
    "            subject_name = os.path.splitext(file)[0]\n",
    "            \n",
    "            # Read data\n",
    "            df = pd.read_csv(os.path.join(emotion_dir, file), skiprows=2)\n",
    "\n",
    "            df = df[df['Emotion'].isin(EMOTIONS)]  # Filter invalid emotions\n",
    "            df['SmoothedEmotion'] = smooth_emotions2(df['Emotion'])  # Smooth emotions\n",
    "            \n",
    "            # Calculate total duration (seconds)\n",
    "            total_duration = len(df) * FRAME_DURATION / 1000\n",
    "            \n",
    "            # Initialize feature dictionary for this subject\n",
    "            subject_features = {'Person': subject_name}\n",
    "\n",
    "            # Calculate GEV features\n",
    "            if feature_type == 'GEV':                \n",
    "                gev_series = df.groupby('SmoothedEmotion').size() / len(df)\n",
    "                for emotion in EMOTIONS:\n",
    "                    gev = gev_series.get(emotion, 0)  # If emotion doesn't exist, return 0\n",
    "                    if emotion == 'happy':\n",
    "                        subject_features[f'GEV_sad'] = gev\n",
    "                    elif emotion == 'sad':\n",
    "                        subject_features[f'GEV_happy'] = gev\n",
    "                    else:\n",
    "                        subject_features[f'GEV_{emotion}'] = gev\n",
    "\n",
    "            # Calculate occurrence frequency (changes per second)\n",
    "            if feature_type == 'Frequency':\n",
    "                emotion_sequence = df['SmoothedEmotion'].values\n",
    "                changes = 0  # Number of emotion changes\n",
    "                # Calculate occurrence frequency for each emotion\n",
    "                for emotion in EMOTIONS:\n",
    "                    changes = 0\n",
    "                    for i in range(1, len(emotion_sequence)):\n",
    "                        if emotion_sequence[i] == emotion and emotion_sequence[i-1] != emotion:\n",
    "                            changes += 1\n",
    "                    # Calculate frequency (number of changes per second)\n",
    "                    frequency = changes / total_duration\n",
    "                    if emotion == 'happy':\n",
    "                        subject_features[f'Frequency_sad'] = frequency\n",
    "                    elif emotion == 'sad':\n",
    "                        subject_features[f'Frequency_happy'] = frequency\n",
    "                    else:\n",
    "                        subject_features[f'Frequency_{emotion}'] = frequency\n",
    "\n",
    "            # Calculate average duration\n",
    "            if feature_type == 'Duration':\n",
    "                # Initialize duration for all emotions to 0\n",
    "                for emotion in EMOTIONS:\n",
    "                    subject_features[f'Duration_{emotion}'] = 0\n",
    "                # Detect emotion change points\n",
    "                emotion_segments = df['SmoothedEmotion'] != df['SmoothedEmotion'].shift()\n",
    "                # Calculate number of segments for each emotion\n",
    "                emotion_runs = df[emotion_segments]['SmoothedEmotion'].value_counts()\n",
    "                # Calculate total frames for each emotion\n",
    "                emotion_total_frames = df.groupby('SmoothedEmotion').size()\n",
    "                # Calculate average duration for each emotion (milliseconds)\n",
    "                for emotion in EMOTIONS:\n",
    "                    if emotion in emotion_runs.index and emotion in emotion_total_frames.index:\n",
    "                        avg_duration = (emotion_total_frames[emotion] / emotion_runs[emotion]) * FRAME_DURATION\n",
    "                        subject_features[f'Duration_{emotion}'] = avg_duration\n",
    "                    else:\n",
    "                        subject_features[f'Duration_{emotion}'] = 0\n",
    "            \n",
    "            # Calculate average recognition probability\n",
    "            if feature_type == 'Probability':\n",
    "                mean_probability = df.groupby('SmoothedEmotion')['Probability'].mean().round(2)\n",
    "                std_probability = df.groupby('SmoothedEmotion')['Probability'].std().round(2)\n",
    "                for emotion in EMOTIONS:\n",
    "                    mean = mean_probability.get(emotion, 0)  # If emotion doesn't exist, return 0\n",
    "                    std = std_probability.get(emotion, 0)\n",
    "                    subject_features[f'Probability_mean_{emotion}'] = mean\n",
    "                    subject_features[f'Probability_std_{emotion}'] = std\n",
    "                \n",
    "            # State transition matrix including same emotions for chord diagram\n",
    "            if feature_type == 'TransWithSelf':\n",
    "                transition_counts = pd.crosstab(\n",
    "                    df['SmoothedEmotion'], \n",
    "                    df['SmoothedEmotion'].shift(-1), \n",
    "                    normalize='index'\n",
    "                )\n",
    "                # Add transition probabilities\n",
    "                for e1 in EMOTIONS:\n",
    "                    for e2 in EMOTIONS:\n",
    "                            subject_features[f'TransWithSelf_{e1}_to_{e2}'] = transition_counts.get(e2, {}).get(e1, 0)\n",
    "\n",
    "            # State transition matrix excluding same emotions for statistical analysis of group differences\n",
    "            if feature_type == 'TransWithOutSelf':\n",
    "                # df = df[df['Emotion'].isin(EMOTIONS)]  # Filter invalid emotions\n",
    "                # df['SmoothedEmotion'] = smooth_emotions2(df['Emotion'])  # Smooth emotions\n",
    "                current_emotions = df['SmoothedEmotion'].iloc[:-1].reset_index(drop=True)  # Remove last one\n",
    "                next_emotions = df['SmoothedEmotion'].iloc[1:].reset_index(drop=True)      # Remove first one\n",
    "                # Only keep transitions between different emotions\n",
    "                mask = current_emotions != next_emotions\n",
    "                current_emotions = current_emotions[mask]\n",
    "                next_emotions = next_emotions[mask]\n",
    "                # Calculate transition probability matrix\n",
    "                transition_counts = pd.crosstab(\n",
    "                    current_emotions,\n",
    "                    next_emotions,\n",
    "                    normalize='index'\n",
    "                )\n",
    "                # Add transition probabilities\n",
    "                for e1 in EMOTIONS:\n",
    "                    for e2 in EMOTIONS:\n",
    "                        subject_features[f'TransWithOutSelf_{e1}_to_{e2}'] = transition_counts.get(e2, {}).get(e1, 0)\n",
    "\n",
    "            # State transition group comparison excluding same emotions, data source for transition plots\n",
    "            if feature_type == 'GroupCompareTransWithOutSelf':\n",
    "                trans_data = analyze_emotion_features(emotion_dir, 'TransWithOutSelf', \n",
    "                                                      remove_outlier=remove_outlier, demographic_csv=demographic_csv, \n",
    "                                                      output_dir=output_dir, is_save = False)\n",
    "                print(trans_data)\n",
    "                # Get data for this group\n",
    "                trans_data_ASD, trans_data_TD = trans_data[trans_data['组别'] == 1], trans_data[trans_data['组别'] == 0]\n",
    "\n",
    "\n",
    "                \n",
    "                EMOTION_ORDER = ['neutral', 'happy', 'sad', 'surprise', 'anger']\n",
    "                n_emotions = len(EMOTION_ORDER)\n",
    "\n",
    "                # Create transition matrix\n",
    "                trans_matrix_data, trans_matrix_p_value = np.zeros((n_emotions, n_emotions)), np.zeros((n_emotions, n_emotions))\n",
    "\n",
    "                # Fill transition matrix\n",
    "                for i, from_emotion in enumerate(EMOTION_ORDER):\n",
    "                    for j, to_emotion in enumerate(EMOTION_ORDER):\n",
    "                        if from_emotion == to_emotion: continue\n",
    "                        col_name = f'TransWithOutSelf_{from_emotion}_to_{to_emotion}'\n",
    "                        asd_data, td_data = trans_data_ASD[col_name], trans_data_TD[col_name]\n",
    "\n",
    "                        # Calculate mean difference\n",
    "                        trans_matrix_data[i, j] = asd_data.mean() - td_data.mean()\n",
    "                        _, p_value = stats.ttest_ind(asd_data, td_data)\n",
    "                        trans_matrix_p_value[i, j] = p_value\n",
    "\n",
    "                if is_save:\n",
    "                    # Convert to DataFrame and add row/column labels\n",
    "                    df_data = pd.DataFrame(trans_matrix_data, \n",
    "                                        index=EMOTION_ORDER, \n",
    "                                        columns=EMOTION_ORDER)\n",
    "                    df_p_value = pd.DataFrame(trans_matrix_p_value, \n",
    "                                            index=EMOTION_ORDER, \n",
    "                                            columns=EMOTION_ORDER)\n",
    "                    output_file = os.path.join(output_dir, 'TransBetweenGroups_analysis(From_Row_to_Col).xlsx')\n",
    "                    with pd.ExcelWriter(output_file) as writer:\n",
    "                        df_data.to_excel(writer, sheet_name='Mean_Difference')\n",
    "                        df_p_value.to_excel(writer, sheet_name='P_Values')\n",
    "                    print(f\"分析结果已保存至：{output_file}\")\n",
    "\n",
    "                return trans_matrix_data, trans_matrix_p_value\n",
    "            \n",
    "            # Add to results list\n",
    "            results.append(subject_features)\n",
    "\n",
    "            # Print progress\n",
    "            print(f\"\\r处理进度：{len(results)}/{len(csv_files)}\", end='')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n处理文件 {file} 时出错: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Read demographic.csv and merge\n",
    "    info_cols = ['姓名', '组别', 'ABC', 'S1', 'R', 'B', 'L', 'S2', '克氏', 'Age']\n",
    "    demographic_df = pd.read_csv(demographic_csv)[info_cols]\n",
    "    results_df = pd.merge(demographic_df, results_df, left_on='姓名', right_on='Person')\n",
    "    results_df = results_df.drop('姓名', axis=1)\n",
    "\n",
    "    feature_cols = [col for col in results_df.columns if col.startswith(feature_type)]\n",
    "    \n",
    "    # Remove outliers from data\n",
    "    if remove_outlier:\n",
    "        for col in feature_cols:\n",
    "            for group in [0, 1]:\n",
    "                mask = results_df['组别'] == group\n",
    "                group_data = results_df.loc[mask, col]  \n",
    "                q1 = group_data.quantile(0.25)\n",
    "                q3 = group_data.quantile(0.75)\n",
    "                iqr = q3 - q1\n",
    "                lower_bound = q1 - 1.5 * iqr\n",
    "                upper_bound = q3 + 1.5 * iqr\n",
    "                results_df.loc[mask & (group_data < lower_bound), col] = np.nanmedian(group_data)\n",
    "                results_df.loc[mask & (group_data > upper_bound), col] = np.nanmedian(group_data)\n",
    "\n",
    "    print(\"\\n处理完成！\")\n",
    "    \n",
    "    if is_save:\n",
    "        # Save results\n",
    "        output_file = os.path.join(output_dir, f'{feature_type}.csv')\n",
    "        results_df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "        print(f\"结果已保存至：{output_file}\")\n",
    "\n",
    "    # # Display basic statistical information\n",
    "    # print(f\"\\n{feature_type} feature statistics:\")\n",
    "    # print(results_df[feature_cols].describe())\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Statistical analysis\n",
    "def statistics_analysis(feature_type, feature_dir=\"result/Emotion_features\"):\n",
    "    \"\"\"\n",
    "    Perform statistical analysis on emotion features, including:\n",
    "    1. Descriptive statistics\n",
    "    2. Normality test\n",
    "    3. Homogeneity of variance test\n",
    "    4. Group difference test (t-test or Mann-Whitney U test)\n",
    "    5. FDR correction\n",
    "    6. Effect size calculation\n",
    "    7. Significance marking\n",
    "    \"\"\"\n",
    "    from statsmodels.stats.multitest import fdrcorrection\n",
    "    \n",
    "    # Define emotion order\n",
    "    EMOTION_ORDER = ['Neutral', 'Happy', 'Sad', 'Surprise', 'Anger']\n",
    "    \n",
    "    # Read data\n",
    "    feature_df = pd.read_csv(os.path.join(feature_dir, f'{feature_type}.csv'))\n",
    "    feature_cols = [col for col in feature_df.columns if col.startswith(feature_type)]\n",
    "\n",
    "    print(feature_cols)\n",
    "    \n",
    "    # Store statistical results\n",
    "    stats_results = []\n",
    "    \n",
    "    for col in feature_cols:\n",
    "        # Get group data\n",
    "        td_data = feature_df[feature_df['组别'] == 0][col]    # TD group (组别=0)\n",
    "        asd_data = feature_df[feature_df['组别'] == 1][col]   # ASD group (组别=1)\n",
    "        \n",
    "        # 1. Descriptive statistics\n",
    "        desc_td = f\"{td_data.mean():.3f}±{td_data.std():.3f}\"\n",
    "        desc_asd = f\"{asd_data.mean():.3f}±{asd_data.std():.3f}\"\n",
    "        \n",
    "        # 2. Normality test (Shapiro-Wilk test)\n",
    "        _, p_normal_td = stats.shapiro(td_data)\n",
    "        _, p_normal_asd = stats.shapiro(asd_data)\n",
    "        is_normal = (p_normal_td > 0.05) and (p_normal_asd > 0.05)\n",
    "        \n",
    "        # 3. Homogeneity of variance test (Levene's test)\n",
    "        _, p_levene = stats.levene(td_data, asd_data)\n",
    "        \n",
    "        # 4. Choose test method based on normality\n",
    "        if is_normal:\n",
    "            t_stat, p_value = stats.ttest_ind(td_data, asd_data, equal_var=(p_levene > 0.05))\n",
    "            test_method = \"t检验\"\n",
    "        else:\n",
    "            u_stat, p_value = stats.mannwhitneyu(td_data, asd_data, alternative='two-sided')\n",
    "            test_method = \"Mann-Whitney U\"\n",
    "        \n",
    "        # Store results\n",
    "        stats_results.append({\n",
    "            '特征': col.replace(f'{feature_type}_', '').capitalize(),  # Simplify feature name\n",
    "            'TD组': desc_td,\n",
    "            'ASD组': desc_asd,\n",
    "            '检验方法': test_method,\n",
    "            'p值': p_value,\n",
    "        })\n",
    "    \n",
    "    # 5. FDR correction\n",
    "    p_values = [result['p值'] for result in stats_results]\n",
    "    _, p_fdr = fdrcorrection(p_values)\n",
    "    \n",
    "    # Update results\n",
    "    for i in range(len(stats_results)):\n",
    "        stats_results[i]['FDR校正p值'] = p_fdr[i]\n",
    "        \n",
    "        # Add significance marking\n",
    "        raw_p = stats_results[i]['p值']\n",
    "        fdr_p = p_fdr[i]\n",
    "        \n",
    "        # Significance marking for raw p-values\n",
    "        if raw_p < 0.001:\n",
    "            raw_sig = '***'\n",
    "        elif raw_p < 0.01:\n",
    "            raw_sig = '**'\n",
    "        elif raw_p < 0.05:\n",
    "            raw_sig = '*'\n",
    "        else:\n",
    "            raw_sig = 'ns'\n",
    "            \n",
    "        # Significance marking for FDR-corrected p-values\n",
    "        if fdr_p < 0.001:\n",
    "            fdr_sig = '***'\n",
    "        elif fdr_p < 0.01:\n",
    "            fdr_sig = '**'\n",
    "        elif fdr_p < 0.05:\n",
    "            fdr_sig = '*'\n",
    "        else:\n",
    "            fdr_sig = 'ns'\n",
    "            \n",
    "        stats_results[i]['显著性'] = f\"{raw_sig}/{fdr_sig}\"\n",
    "    \n",
    "    # Convert to DataFrame and format\n",
    "    results_df = pd.DataFrame(stats_results)\n",
    "    \n",
    "    # Format p-values\n",
    "    for col in ['p值', 'FDR校正p值']:\n",
    "        results_df[col] = results_df[col].apply(lambda x: f\"{x:.3f}\" if x >= 0.001 else \"<0.001\")\n",
    "\n",
    "    # Create categorical type for sorting\n",
    "    results_df['特征'] = pd.Categorical(\n",
    "        results_df['特征'], \n",
    "        categories=EMOTION_ORDER, \n",
    "        ordered=True\n",
    "    )\n",
    "    \n",
    "    # Sort by specified order\n",
    "    results_df = results_df.sort_values('特征')\n",
    "\n",
    "    # Set column names\n",
    "    results_df.columns = ['特征', 'TD组 (M±SD)', 'ASD组 (M±SD)', '检验方法', 'p值', 'FDR校正p值', '显著性']\n",
    "    \n",
    "    # Print results table\n",
    "    print(f\"\\n{feature_type}特征组间差异分析结果：\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # Set pandas display options\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 120)\n",
    "    pd.set_option('display.max_colwidth', 30)\n",
    "    pd.set_option('display.colheader_justify', 'center')\n",
    "    \n",
    "    # Use tabulate to print aligned table\n",
    "    from tabulate import tabulate\n",
    "    print(tabulate(results_df, headers='keys', tablefmt='pipe', showindex=False))\n",
    "    \n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "def plot_emotion_bars(feature_type, feature_dir=\"result/Emotion_features\", show_points=True, save_fig=False):\n",
    "    \"\"\"\n",
    "    Plot grouped bar chart for emotion features\n",
    "    Parameters:\n",
    "        feature_type: Feature type (e.g. 'GEV')\n",
    "        feature_dir: Feature data directory\n",
    "        show_points: Whether to show scatter points\n",
    "        save_fig: Whether to save figure\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Set Chinese font\n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei']  # For normal display of Chinese labels\n",
    "    plt.rcParams['axes.unicode_minus'] = False     # For normal display of negative signs\n",
    "    \n",
    "    # Define emotion order and Chinese labels\n",
    "    EMOTION_ORDER = ['Neutral', 'Happy', 'Sad', 'Surprise', 'Anger']\n",
    "    EMOTION_LABELS = ['中性', '快乐', '悲伤', '惊讶', '愤怒']\n",
    "    EMOTION_DICT = dict(zip(EMOTION_ORDER, EMOTION_LABELS))\n",
    "    \n",
    "    # Read data\n",
    "    df = pd.read_csv(os.path.join(feature_dir, f'{feature_type}.csv'))\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Set bar chart positions\n",
    "    bar_width = 0.35\n",
    "    x = np.arange(len(EMOTION_ORDER))\n",
    "    \n",
    "    # Store means and standard errors\n",
    "    means_td = []\n",
    "    means_asd = []\n",
    "    sems_td = []\n",
    "    sems_asd = []\n",
    "    \n",
    "    # Calculate mean and standard error for each emotion\n",
    "    for emotion in EMOTION_ORDER:\n",
    "        col = f'{feature_type}_{emotion.lower()}'\n",
    "        \n",
    "        # TD group (组别=0)\n",
    "        td_data = df[df['组别'] == 0][col]\n",
    "        means_td.append(td_data.mean())\n",
    "        sems_td.append(td_data.std() / np.sqrt(len(td_data)))\n",
    "        \n",
    "        # ASD group (组别=1)\n",
    "        asd_data = df[df['组别'] == 1][col]\n",
    "        means_asd.append(asd_data.mean())\n",
    "        sems_asd.append(asd_data.std() / np.sqrt(len(asd_data)))\n",
    "    \n",
    "    # Set colors\n",
    "    colors = ['#9AC9DB', '#2878B5']  # Light blue (TD), Dark blue (ASD)\n",
    "    \n",
    "    # Draw bar chart - TD group on left, ASD group on right\n",
    "    plt.bar(x - bar_width/2, means_td, bar_width, label='TD组',\n",
    "            yerr=sems_td, capsize=5, color=colors[0], alpha=0.8)\n",
    "    plt.bar(x + bar_width/2, means_asd, bar_width, label='ASD组',\n",
    "            yerr=sems_asd, capsize=5, color=colors[1], alpha=0.8)\n",
    "    \n",
    "    # Add scatter points (if needed)\n",
    "    if show_points:\n",
    "        for i, emotion in enumerate(EMOTION_ORDER):\n",
    "            col = f'{feature_type}_{emotion.lower()}'\n",
    "            \n",
    "            # TD group scatter points (left)\n",
    "            td_data = df[df['组别'] == 0][col]\n",
    "            plt.scatter(np.repeat(i - bar_width/2, len(td_data)), \n",
    "                       td_data, \n",
    "                       color=colors[0], \n",
    "                       alpha=0.3, \n",
    "                       s=30)\n",
    "            \n",
    "            # ASD group scatter points (right)\n",
    "            asd_data = df[df['组别'] == 1][col]\n",
    "            plt.scatter(np.repeat(i + bar_width/2, len(asd_data)), \n",
    "                       asd_data, \n",
    "                       color=colors[1], \n",
    "                       alpha=0.3, \n",
    "                       s=30)\n",
    "    \n",
    "    # Add grid lines\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Set figure properties\n",
    "    plt.xlabel('情绪类别', fontsize=12)\n",
    "    plt.ylabel(f'{feature_type}值', fontsize=12)\n",
    "    plt.title(f'{feature_type}特征在TD组和ASD组的对比', fontsize=14, pad=20)\n",
    "    \n",
    "    # Set x-axis ticks\n",
    "    plt.xticks(x, [EMOTION_DICT[emotion] for emotion in EMOTION_ORDER])\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(loc='upper right')\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    if save_fig:\n",
    "        # Ensure directory exists\n",
    "        os.makedirs('result/figures', exist_ok=True)\n",
    "        plt.savefig(f'result/figures/{feature_type}_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    # Display figure\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_transition_matrix(feature_type, group, include_self_transition=False, is_standardized=False, feature_dir=\"result/Emotion_features\"):\n",
    "    \"\"\"\n",
    "    Plot state transition matrix heatmap for specified group\n",
    "    \n",
    "    Parameters:\n",
    "        feature_type: Feature type\n",
    "        group: Group (0 or 1)\n",
    "        feature_dir: Feature data directory\n",
    "    \"\"\"\n",
    "    emotions = ['neutral', 'happy', 'sad', 'surprise', 'anger']\n",
    "    n_emotions = len(emotions)\n",
    "    \n",
    "    # Create transition matrix\n",
    "    trans_matrix = np.zeros((n_emotions, n_emotions))\n",
    "    \n",
    "    # Get data for this group\n",
    "    df = pd.read_csv(os.path.join(feature_dir, f'{feature_type}.csv'))\n",
    "    group_df = df[df['组别'] == group]\n",
    "    \n",
    "    # Fill transition matrix\n",
    "    for i, from_emotion in enumerate(emotions):\n",
    "        for j, to_emotion in enumerate(emotions):\n",
    "            if not include_self_transition and from_emotion == to_emotion:\n",
    "                    continue\n",
    "            col_name = f'{feature_type}_{from_emotion}_to_{to_emotion}'\n",
    "            trans_matrix[i, j] = group_df[col_name].mean()\n",
    "\n",
    "    # Sinkhorn-Knopp algorithm for bidirectional standardization\n",
    "    if is_standardized:\n",
    "        tolerance = 1e-10\n",
    "        max_iter = 1000\n",
    "        \n",
    "        for _ in range(max_iter):\n",
    "            # Row standardization\n",
    "            row_sums = trans_matrix.sum(axis=1, keepdims=True)\n",
    "            row_sums[row_sums == 0] = 1  # Avoid division by zero\n",
    "            trans_matrix = trans_matrix / row_sums\n",
    "            \n",
    "            # Column standardization\n",
    "            col_sums = trans_matrix.sum(axis=0, keepdims=True)\n",
    "            col_sums[col_sums == 0] = 1  # Avoid division by zero\n",
    "            trans_matrix = trans_matrix / col_sums\n",
    "            \n",
    "            # Check for convergence\n",
    "            if np.all(np.abs(trans_matrix.sum(axis=1) - 1) < tolerance) and \\\n",
    "               np.all(np.abs(trans_matrix.sum(axis=0) - 1) < tolerance):\n",
    "                break\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(trans_matrix, \n",
    "                annot=True,  # Show values\n",
    "                fmt='.3f',   # Value format to 3 decimal places\n",
    "                cmap='YlOrRd',  # Use YlOrRd color scheme\n",
    "                xticklabels=emotions,\n",
    "                yticklabels=emotions,\n",
    "                vmin=0, \n",
    "                vmax=1)\n",
    "    \n",
    "    plt.title(f'组{group}的{feature_type}特征转移概率矩阵')\n",
    "    plt.xlabel('转移到')\n",
    "    plt.ylabel('转移自')\n",
    "    \n",
    "    # Set Chinese font\n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. GEV features\n",
    "# analyze_emotion_features(emotion_dir=\"result/Emotions\", feature_type=\"GEV\", remove_outlier=False)\n",
    "# statistics_analysis(feature_type=\"GEV\")\n",
    "# plot_emotion_bars(feature_type=\"GEV\", show_points=False)\n",
    "\n",
    "# # 2. Frequency features\n",
    "# analyze_emotion_features(emotion_dir=\"result/Emotions_smooth2\", feature_type=\"Frequency\", remove_outlier=False)\n",
    "# statistics_analysis(feature_type=\"Frequency\")\n",
    "# plot_emotion_bars(feature_type=\"Frequency\", show_points=False)\n",
    "\n",
    "# # # 3. Duration features\n",
    "# analyze_emotion_features(emotion_dir=\"result/Emotions_smooth2\", feature_type=\"Duration\", remove_outlier=False)\n",
    "# statistics_analysis(feature_type=\"Duration\")\n",
    "# plot_emotion_bars(feature_type=\"Duration\", show_points=False)\n",
    "\n",
    "# 4. Recognition probability\n",
    "# analyze_emotion_features(emotion_dir=\"result/Emotions\", feature_type=\"Probability\", remove_outlier=False)\n",
    "# statistics_analysis(feature_type=\"Probability\")\n",
    "# plot_emotion_bars(feature_type=\"Probability\", show_points=False)\n",
    "\n",
    "# # 5. State transition matrix for chord diagram\n",
    "# analyze_emotion_features(emotion_dir=\"result/Emotions_smooth2\", feature_type=\"TransWithSelf\", remove_outlier=False)\n",
    "# plot_transition_matrix(feature_type=\"TransWithSelf\", group=0, is_standardized=True)\n",
    "\n",
    "# # 6. State transition matrix excluding same emotions for statistical analysis of group differences\n",
    "# analyze_emotion_features(emotion_dir=\"result/Emotions_smooth2\", feature_type=\"TransWithOutSelf\", remove_outlier=False)\n",
    "# plot_transition_matrix(feature_type=\"TransWithOutSelf\", group=0)\n",
    "\n",
    "# # 6.1. Statistical comparison of state transition differences\n",
    "# analyze_emotion_features(emotion_dir=\"result/Emotions_smooth2\", feature_type=\"GroupCompareTransWithOutSelf\", remove_outlier=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征汇总完成，已保存至：result/machine_learning/merged_emotion_features_0.csv\n",
      "原始数据行数: 184\n",
      "合并后数据行数: 184\n",
      "(184, 44)\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "def merge_emotion_features():\n",
    "    # Read various feature files\n",
    "    gev_df = pd.read_csv('result/Emotion_features/GEV.csv')\n",
    "    duration_df = pd.read_csv('result/Emotion_features/Duration.csv')\n",
    "    frequency_df = pd.read_csv('result/Emotion_features/Frequency.csv')\n",
    "    probability_df = pd.read_csv('result/Emotion_features/Probability.csv')\n",
    "    trans_df = pd.read_csv('result/Emotion_features/TransWithOutSelf.csv')\n",
    "    \n",
    "    # Use basic information columns from duration_df as base\n",
    "    base_cols = ['组别', 'ABC', 'S1', 'R', 'B', 'L', 'S2', '克氏', 'Age', 'Person']\n",
    "    result_df = duration_df[base_cols]\n",
    "    \n",
    "    # GEV features\n",
    "    result_df = result_df.merge(\n",
    "        gev_df.drop(columns=base_cols),\n",
    "        left_on='Person',\n",
    "        right_on=gev_df['Person'],\n",
    "        validate='1:1'\n",
    "    )\n",
    "\n",
    "    # Use merge method to combine data based on Person column\n",
    "    # Duration features\n",
    "    result_df = result_df.merge(\n",
    "        duration_df.drop(columns=base_cols), \n",
    "        left_index=True, \n",
    "        right_index=True,\n",
    "        validate='1:1'\n",
    "    )\n",
    "    \n",
    "    # Frequency features\n",
    "    result_df = result_df.merge(\n",
    "        frequency_df.drop(columns=base_cols),\n",
    "        left_on='Person',\n",
    "        right_on=frequency_df['Person'],\n",
    "        validate='1:1'\n",
    "    )\n",
    "\n",
    "    # # probability features\n",
    "    # result_df = result_df.merge(\n",
    "    #     probability_df.drop(columns=base_cols),\n",
    "    #     left_on='Person',\n",
    "    #     right_on=probability_df['Person'],\n",
    "    #     validate='1:1'\n",
    "    # )\n",
    "    \n",
    "    # Transition probability features\n",
    "    result_df = result_df.merge(\n",
    "        trans_df.drop(columns=base_cols),\n",
    "        left_on='Person',\n",
    "        right_on=trans_df['Person'],\n",
    "        validate='1:1'\n",
    "    )\n",
    "\n",
    "    # Rename columns\n",
    "    result_df = result_df.rename(columns={\n",
    "        'Person': '姓名',\n",
    "        '组别': 'group'\n",
    "    })\n",
    "\n",
    "\n",
    "    # Remove unnecessary columns\n",
    "    cols_to_drop = ['S1', 'R', 'B', 'L', 'S2', 'Age']\n",
    "    result_df = result_df.drop(columns=cols_to_drop)\n",
    "\n",
    "    # Reorder basic information columns in specified order\n",
    "    ordered_cols = ['姓名', 'group', 'ABC', '克氏']\n",
    "    other_cols = [col for col in result_df.columns if col not in ordered_cols]\n",
    "    result_df = result_df[ordered_cols + other_cols]\n",
    "\n",
    "    # # Sort by first letter of name\n",
    "    # result_df = result_df.sort_values(by='姓名', key=lambda x: x.str[0])\n",
    "    \n",
    "    # Save summary results\n",
    "    output_file = 'result/machine_learning/merged_emotion_features.csv'\n",
    "    result_df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "    print(f\"特征汇总完成，已保存至：{output_file}\")\n",
    "    \n",
    "    # Check for data loss\n",
    "    print(f\"原始数据行数: {len(duration_df)}\")\n",
    "    print(f\"合并后数据行数: {len(result_df)}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Execute summary\n",
    "merged_df = merge_emotion_features()\n",
    "print(merged_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ASD_Face_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
